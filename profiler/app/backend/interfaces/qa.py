"""
Interface definitions for QA service.

This module defines the interfaces for the QA (Question and Answer) service.
"""

from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional


class Question:
    """Represents a question generated by the QA service."""
    
    def __init__(
        self,
        question_id: str,
        question_text: str,
        category: str,
        expected_response_type: str = "text",
        required: bool = True,
        metadata: Optional[Dict[str, Any]] = None
    ):
        """
        Initialize a Question object.
        
        Args:
            question_id: Unique identifier for the question
            question_text: The text of the question
            category: The category or topic of the question
            expected_response_type: The expected type of response (text, number, etc.)
            required: Whether an answer to this question is required
            metadata: Additional metadata for the question
        """
        self.question_id = question_id
        self.question_text = question_text
        self.category = category
        self.expected_response_type = expected_response_type
        self.required = required
        self.metadata = metadata or {}
    
    def dict(self) -> Dict[str, Any]:
        """
        Convert the Question object to a dictionary.
        
        Returns:
            A dictionary representation of the Question
        """
        return {
            "question_id": self.question_id,
            "question": self.question_text,
            "category": self.category,
            "expected_response_type": self.expected_response_type,
            "required": self.required,
            "metadata": self.metadata
        }


class AnswerEvaluation:
    """Represents an evaluation of an answer to a question."""
    
    def __init__(
        self,
        question_id: str,
        response: str,
        confidence: float,
        quality_score: float,
        feedback: Optional[str] = None
    ):
        """
        Initialize an AnswerEvaluation object.
        
        Args:
            question_id: ID of the question that was answered
            response: The user's response
            confidence: Confidence score for the evaluation (0-1)
            quality_score: Quality score for the answer (0-1)
            feedback: Optional feedback on the answer
        """
        self.question_id = question_id
        self.response = response
        self.confidence = confidence
        self.quality_score = quality_score
        self.feedback = feedback
    
    def dict(self) -> Dict[str, Any]:
        """
        Convert the AnswerEvaluation object to a dictionary.
        
        Returns:
            A dictionary representation of the AnswerEvaluation
        """
        result = {
            "question_id": self.question_id,
            "response": self.response,
            "confidence": self.confidence,
            "quality_score": self.quality_score,
        }
        
        if self.feedback:
            result["feedback"] = self.feedback
            
        return result


class IQAService(ABC):
    """Interface for QA services."""
    
    @abstractmethod
    def configure(self, config: Dict[str, Any]) -> None:
        """
        Configure the service with the provided settings.
        
        Args:
            config: Configuration parameters for the service
        """
        pass
    
    @abstractmethod
    def generate_questions(
        self,
        input_text: str,
        profile_data: Optional[Dict[str, Any]] = None,
        context: Optional[Dict[str, Any]] = None,
        num_questions: int = 3
    ) -> List[Question]:
        """
        Generate questions based on input text and context.
        
        Args:
            input_text: The input text to generate questions from
            profile_data: Optional profile data to contextualize questions
            context: Optional context information (e.g., conversation history)
            num_questions: Number of questions to generate
            
        Returns:
            A list of Question objects
        """
        pass
    
    @abstractmethod
    def evaluate_answer(
        self,
        question: Question,
        answer: str,
        profile_data: Optional[Dict[str, Any]] = None
    ) -> AnswerEvaluation:
        """
        Evaluate an answer to a specific question.
        
        Args:
            question: The Question that was answered
            answer: The user's answer
            profile_data: Optional profile data for context
            
        Returns:
            An AnswerEvaluation object with the evaluation results
        """
        pass 